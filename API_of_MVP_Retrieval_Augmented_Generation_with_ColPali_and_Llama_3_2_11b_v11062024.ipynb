{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/llama11b-vision-rag-api/blob/main/API_of_MVP_Retrieval_Augmented_Generation_with_ColPali_and_Llama_3_2_11b_v11062024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjzTdJDThT_7"
      },
      "source": [
        "### 0.Requirement and Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uePV8K4IkQh"
      },
      "outputs": [],
      "source": [
        "!pip install pymilvus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqrrGGAGFUNI"
      },
      "outputs": [],
      "source": [
        "!pip install colpali_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2RQG17bhbKU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "# from colpali_engine.utils import process_images, process_queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjTDHspeFQZ0"
      },
      "outputs": [],
      "source": [
        "# !pip install colpali_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0s3_q6ZFQZ1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPnqYAbHdoJJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor, TextStreamer\n",
        "from IPython.display import Markdown,display\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17gLCV47FQZ1"
      },
      "outputs": [],
      "source": [
        "# Colpali Model\n",
        "from colpali_engine.models import ColPali\n",
        "\n",
        "# Colpali queries and images preprocessing\n",
        "from colpali_engine.models.paligemma.colpali.processing_colpali import ColPaliProcessor\n",
        "\n",
        "# Retruever Processor\n",
        "from colpali_engine.utils.processing_utils import BaseVisualRetrieverProcessor\n",
        "\n",
        "# Accelerate calculations\n",
        "from colpali_engine.utils.torch_utils import ListDataset, get_torch_device\n",
        "\n",
        "# Pytorch Data Loader Object\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Pytorch Library\n",
        "import torch\n",
        "\n",
        "# Type Validation\n",
        "from typing import List, cast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHGSLyE1FQZ2"
      },
      "source": [
        "#### Check the availability of GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri9RVC8qFQZ3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3tYuzuFQZ5"
      },
      "source": [
        "### 1.Embedding Processus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv3LXHoKFQZ6"
      },
      "source": [
        "#### 1.0. Downloading and Configuring the Colpali Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4YeOaNZFQZ6"
      },
      "outputs": [],
      "source": [
        "device = get_torch_device('auto')\n",
        "model_name = \"vidore/colpali-v1.2\"\n",
        "model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=device,\n",
        ").eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeyEXQL7Mu4N"
      },
      "outputs": [],
      "source": [
        "# Used to process queries and images to fit the model's input requirements beforehand\n",
        "processor = cast(ColPaliProcessor, ColPaliProcessor.from_pretrained(model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKVWocaFQZ6"
      },
      "source": [
        "#### 1.1 Queries Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBYj5k_sFQZ7"
      },
      "outputs": [],
      "source": [
        "# Embed the queries.\n",
        "def queries_embedding(queries : list):\n",
        "  # Create a DataLoader to iterate over the list of queries, processing each query\n",
        "  # individually to fit model input requirements\n",
        "  dataloader = DataLoader(\n",
        "      dataset=ListDataset[str](queries),\n",
        "      batch_size=1,\n",
        "      shuffle=False,\n",
        "      collate_fn=lambda x: processor.process_queries(x),\n",
        "  )\n",
        "  qs: List[torch.Tensor] = []\n",
        "\n",
        "  for batch_query in dataloader:\n",
        "      with torch.no_grad():\n",
        "          batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n",
        "          embeddings_query = model(**batch_query)\n",
        "      qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
        "  return qs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ1oo1V0FQZ9"
      },
      "source": [
        "#### 1.2. Document Embedding Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20gGiVIXIjdC"
      },
      "outputs": [],
      "source": [
        "# !pip install pdf2image\n",
        "# !pip install pdf2jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPTY6kpOiieh"
      },
      "outputs": [],
      "source": [
        "from pymilvus import connections, Collection, utility\n",
        "from pdf2jpg import pdf2jpg\n",
        "from pdf2image import convert_from_path\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bWXDUbzFQZ9"
      },
      "source": [
        "##### 1.2.1. Convert PDF to PNG images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o07v2RyNJzOF"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LsuR7fpKJIC"
      },
      "outputs": [],
      "source": [
        "!pdfinfo --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiteLoahFQZ9"
      },
      "outputs": [],
      "source": [
        "# Document Path\n",
        "pdf_path = \"/content/VRAG_Test_Documents.pdf\"\n",
        "\n",
        "# The Output Folder Path\n",
        "output_folder = \"/content/\"\n",
        "def conver_pdf2image(pdf_path, output_folder):\n",
        "\n",
        "    # Images List\n",
        "    images = convert_from_path(pdf_path=pdf_path, output_folder=output_folder, dpi=300, fmt=\"jpg\")\n",
        "\n",
        "    # Extract Path\n",
        "    basename = os.path.splitext(os.path.basename(pdf_path))\n",
        "    # basename[0]\n",
        "\n",
        "    # Save each page as an image\n",
        "    for page_num, page in enumerate(images, start=1):\n",
        "        image_name = f\"{basename}_page_{page_num}.png\"\n",
        "        image_path = os.path.join(output_folder,image_name)\n",
        "        page.save(image_path,\"PNG\")\n",
        "    # Save each page as an image\n",
        "    for page_num, page in enumerate(images, start=1):\n",
        "        image_name = f\"{basename}_page_{page_num}.png\"\n",
        "        image_path = os.path.join(output_folder,image_name)\n",
        "        page.save(image_path,\"PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGMESn-yFQZ9"
      },
      "outputs": [],
      "source": [
        "conver_pdf2image(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6qbTX64FQZ-"
      },
      "outputs": [],
      "source": [
        "image_folder = \"/content\"\n",
        "image_list = []\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "        file_path = os.path.join(image_folder, filename)\n",
        "        image = Image.open(file_path)\n",
        "        image_list.append(image)\n",
        "image_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9KKvz0DFQZ-"
      },
      "outputs": [],
      "source": [
        "image_list[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84vKykhdFQZ-"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(\n",
        "    dataset=ListDataset[str](image_list),\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: processor.process_images(x),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2kq9MUHFQZ-"
      },
      "outputs": [],
      "source": [
        "ds: List[torch.Tensor] = []\n",
        "for batch_doc in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "        # Move all tensors in batch_doc to the same device as the model weights,\n",
        "        # but keep 'input_ids' as Long or Int.\n",
        "        batch_doc = {\n",
        "            k: v.to(dtype=torch.bfloat16, device=device) if k != \"input_ids\" else v.to(device=device)\n",
        "            for k, v in batch_doc.items()\n",
        "        }\n",
        "        embeddings_doc = model(**batch_doc)\n",
        "    ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8fW6NZ1x3Aw"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hX2x_rM6yCr"
      },
      "source": [
        "### 2.Retrieval Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDLp3hcQZKB"
      },
      "source": [
        "#### 2.0. Set up connection to Milvus DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSYKBJBt_lsq"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient, DataType, connections, utility\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from pymilvus import Collection, connections\n",
        "collection_name = \"VRAG_BRD_Content\"\n",
        "client = MilvusClient(uri=\"tcp://0.tcp.in.ngrok.io:14024\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEHYCHSKTZ7b"
      },
      "outputs": [],
      "source": [
        "connections.connect(\"default\", host=\"0.tcp.in.ngrok.io\", port=\"14024\")\n",
        "collection = Collection(collection_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNXufwyiFQZ_"
      },
      "source": [
        "#### 2.1. Create a collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shkibhokFQZ_"
      },
      "outputs": [],
      "source": [
        "client.list_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdTnc9KuFQZ_"
      },
      "outputs": [],
      "source": [
        "# Initialize the retriever with a Milvus client, collection name, and dimensionality of the vector embeddings.\n",
        "# If the collection exists, load it.\n",
        "collection_name = \"VRAG_BRD_Content\"\n",
        "# if client.has_collection(collection_name=collection_name):\n",
        "#     client.load_collection(collection_name)\n",
        "dim = 128\n",
        "\n",
        "if client.has_collection(collection_name=collection_name):\n",
        "    client.drop_collection(collection_name=collection_name)\n",
        "\n",
        "schema = client.create_schema(\n",
        "    auto_id=True,\n",
        "    enable_dynamic_fields=True,\n",
        ")\n",
        "schema.add_field(field_name=\"pk\", datatype=DataType.INT64, is_primary=True)\n",
        "schema.add_field(\n",
        "    field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim\n",
        ")\n",
        "schema.add_field(field_name=\"seq_id\", datatype=DataType.INT16)\n",
        "schema.add_field(field_name=\"doc_id\", datatype=DataType.INT64)\n",
        "\n",
        "# The page path is stored here.\n",
        "schema.add_field(field_name=\"doc\", datatype=DataType.VARCHAR, max_length=65535)\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=collection_name, schema=schema\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7VxAuCWUjzn"
      },
      "outputs": [],
      "source": [
        "client.list_collections()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqRvJ3BFQaA"
      },
      "source": [
        "#### 2.2. Create an index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmcOst1kUmjI"
      },
      "outputs": [],
      "source": [
        "collection.num_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lnNDpcgFQaA"
      },
      "outputs": [],
      "source": [
        "# Release from Memory\n",
        "client.release_collection(collection_name=collection_name)\n",
        "#\n",
        "client.drop_index(\n",
        "    collection_name=collection_name, index_name=\"vector\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqpW3Tp_FQaB"
      },
      "source": [
        "[Index Algorithms](https://milvus.io/docs/index.md?tab=floating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCWuOWCOFQaB"
      },
      "outputs": [],
      "source": [
        "index_params = client.prepare_index_params()\n",
        "\n",
        "index_params.add_index(\n",
        "    field_name=\"vector\",\n",
        "    index_name=\"vector_index\",\n",
        "    index_type=\"HNSW\",  # or any other index type you want\n",
        "    metric_type=\"IP\",  # or the appropriate metric type\n",
        "    params={\n",
        "        \"M\": 16,\n",
        "        \"efConstruction\": 500,\n",
        "    },  # adjust these parameters as needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myhlU4xQU02C"
      },
      "outputs": [],
      "source": [
        "collection_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SZNlTOaFQaH"
      },
      "outputs": [],
      "source": [
        "client.create_index(\n",
        "    collection_name=collection_name, index_params=index_params, sync=True #to be certain that the indexing process has completed before performing further operations\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "164F_N4iFQaH"
      },
      "outputs": [],
      "source": [
        "collection.indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vz1ZvlFQaI"
      },
      "source": [
        "#### 2.3. Insert Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRDOudaMFQaI"
      },
      "outputs": [],
      "source": [
        "filepaths = []\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "        file_path = os.path.join(image_folder, filename)\n",
        "        filepaths.append(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij_BBWaIFQaJ"
      },
      "outputs": [],
      "source": [
        "def insert(self, data):\n",
        "    # Insert ColBERT embeddings and metadata for a document into the collection.\n",
        "    # Embeddings List\n",
        "    colbert_vecs = [vec for vec in data[\"colbert_vecs\"]]\n",
        "\n",
        "    # Patches length\n",
        "    seq_length = len(colbert_vecs)\n",
        "\n",
        "    doc_ids = [data[\"doc_id\"] for i in range(seq_length)]\n",
        "\n",
        "    #\n",
        "    seq_ids = list(range(seq_length))\n",
        "\n",
        "    #\n",
        "    # docs = [\"\"] * seq_length\n",
        "\n",
        "    #\n",
        "    # docs[0] = data[\"filepath\"]\n",
        "\n",
        "    # To be tested :\n",
        "    docs = data[\"filepath\"]\n",
        "\n",
        "\n",
        "    # Insert the data as multiple vectors (one for each sequence) along with the corresponding metadata.\n",
        "    client.insert(\n",
        "        collection_name,\n",
        "        [\n",
        "            {\n",
        "                \"vector\": colbert_vecs[i],\n",
        "                \"seq_id\": seq_ids[i],\n",
        "                \"doc_id\": doc_ids[i],\n",
        "                \"doc\": docs[i],\n",
        "            }\n",
        "            for i in range(seq_length)\n",
        "        ],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv-Qy1j7FQaJ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(filepaths)):\n",
        "    data = {\n",
        "        \"colbert_vecs\": ds[i].float().numpy(),\n",
        "        \"doc_id\": i,\n",
        "        \"filepath\": filepaths[i], #*******************************************************************************************************\n",
        "    }\n",
        "\n",
        "    # Insert ColBERT embeddings and metadata for a document into the collection.\n",
        "    # Embeddings List\n",
        "    colbert_vecs = [vec for vec in data[\"colbert_vecs\"]]\n",
        "\n",
        "    # Patches length\n",
        "    seq_length = len(colbert_vecs)\n",
        "\n",
        "    doc_ids = [data[\"doc_id\"] for i in range(seq_length)]\n",
        "\n",
        "    #\n",
        "    seq_ids = list(range(seq_length))\n",
        "\n",
        "    # #\n",
        "    # docs = [\"\"] * seq_length\n",
        "\n",
        "    #\n",
        "    docs = [data[\"filepath\"]] * seq_length\n",
        "\n",
        "    # Insert the data as multiple vectors (one for each sequence) along with the corresponding metadata.\n",
        "    client.insert(\n",
        "        collection_name,\n",
        "        [\n",
        "            {\n",
        "                \"vector\": colbert_vecs[i],\n",
        "                \"seq_id\": seq_ids[i],\n",
        "                \"doc_id\": doc_ids[i],\n",
        "                \"doc\": docs[i],\n",
        "            }\n",
        "            for i in range(seq_length)\n",
        "        ],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGNR3STKFQaJ"
      },
      "source": [
        "#### Optional: Check the Data in Milvus Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaU1mMjdFQaK"
      },
      "outputs": [],
      "source": [
        "# collection.load()\n",
        "# collection.query(expr=\"pk >= 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEzFb6SUFQaK"
      },
      "outputs": [],
      "source": [
        "collection.flush()\n",
        "collection.num_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSJv7oykFQaK"
      },
      "source": [
        "#### 2.4. Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TgYeFXSwYKl"
      },
      "outputs": [],
      "source": [
        "topk = 3\n",
        "collection_name = \"VRAG_BRD_Content\"\n",
        "search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
        "collection.load()\n",
        "\n",
        "def retriever(qs, collection_name, topk=3, search_params= search_params):\n",
        "  def rerank_single_doc(doc_id, data, client, collection_name):\n",
        "    # Rerank a single document by retrieving its embeddings and calculating the similarity with the query.\n",
        "    doc_colbert_vecs = client.query(\n",
        "        collection_name=collection_name,\n",
        "        filter=f\"doc_id in [{doc_id}, {doc_id + 1}]\",\n",
        "        output_fields=[\"seq_id\", \"vector\", \"doc\"],\n",
        "        limit=1000,\n",
        "    )\n",
        "    doc_vecs = np.vstack(\n",
        "        [doc_colbert_vecs[i][\"vector\"] for i in range(len(doc_colbert_vecs))]\n",
        "    )\n",
        "    score = np.dot(data, doc_vecs.T).max(1).sum()\n",
        "    return (score, doc_id)\n",
        "    #***************************************************************************\n",
        "\n",
        "  images_paths = []\n",
        "  for query in qs:\n",
        "      #0. Get all the documents that contain at least 1 similar (token-patch)\n",
        "      query = query.float().numpy()\n",
        "      results = client.search(\n",
        "          collection_name,\n",
        "          query,\n",
        "          limit=5,\n",
        "          output_fields=[\"vector\", \"seq_id\", \"doc_id\"],\n",
        "          search_params=search_params,\n",
        "      )\n",
        "\n",
        "      #1. Retrieve all document IDs that contain at least one similarity between the query tokens and the document patches\n",
        "      doc_ids = set()\n",
        "      for r_id in range(len(results)): # len(number of tokens) : for each query token\n",
        "          for r in range(len(results[r_id])): # for each similar patch\n",
        "              doc_ids.add(results[r_id][r][\"entity\"][\"doc_id\"]) # add the document_id to the list\n",
        "      # print(doc_ids)\n",
        "\n",
        "      #2. Get the maximum similarity score for each query across all documents :\n",
        "      # Example: The maximum score for the first query with image one is 20, while the maximum score with image two is 12.\n",
        "      # Therefore, image one is more similar to the query than the other images.\n",
        "\n",
        "      #2.1. Create the similarity search function\n",
        "      scores = []\n",
        "      #2.2. # Run the rerank(document) task in parallel for up to 300 workers\n",
        "      with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:\n",
        "              futures = {\n",
        "                  executor.submit(\n",
        "                      rerank_single_doc, doc_id, query, client, collection_name\n",
        "                  ): doc_id\n",
        "                  for doc_id in doc_ids\n",
        "              }\n",
        "              for future in concurrent.futures.as_completed(futures):\n",
        "                  score, doc_id = future.result()\n",
        "                  scores.append((score, doc_id))\n",
        "\n",
        "      scores.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "      if len(scores) >= topk:\n",
        "            scores = scores[:topk]\n",
        "      else:\n",
        "            scores = scores\n",
        "\n",
        "      for i in scores:\n",
        "        image_path = collection.query(expr=f\"doc_id == {i[-1]}\", output_fields=[\"doc\"], limit=1)[0]['doc']\n",
        "        images_paths.append(image_path)\n",
        "\n",
        "      return images_paths, scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = queries_embedding([\"List of business definitions ?\"])"
      ],
      "metadata": {
        "id": "mcSOQGUzUzN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs"
      ],
      "metadata": {
        "id": "ubocLAtqU4q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_paths, scores = retriever(qs = qs, collection_name=collection_name)"
      ],
      "metadata": {
        "id": "1GVF0OMVUf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_paths"
      ],
      "metadata": {
        "id": "VPYaWVneUkHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04obfhUSY50F"
      },
      "source": [
        "#### 2.5. Display the top similar results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvqosPsHLlT3"
      },
      "outputs": [],
      "source": [
        "for i in images_paths:\n",
        "  img = Image.open(i)\n",
        "  display(img)\n",
        "  print(\"*\"*150)\n",
        "  print(\"*\"*150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYHki8MKFfyT"
      },
      "source": [
        "### 4. VModel utilisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg9fwx85gzEw"
      },
      "source": [
        "#### 4.1. unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEdrcLuVg7Mu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF2YI459iOUQ"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeLaL0Ddh7fs"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_uJAcwBg3II"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-oKGhCd4q4S"
      },
      "outputs": [],
      "source": [
        "torch.is_bitsandbytes_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPPR1YMK5HtD"
      },
      "source": [
        "#### 4.2. Llama 3.2 11b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qzk6w5RS5xaY"
      },
      "outputs": [],
      "source": [
        "# image_path = images_paths[0:3]\n",
        "# image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8b-cO49tg1b"
      },
      "outputs": [],
      "source": [
        "#0. Setup the token to download the model\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"*********************************\"  # Replace with your actual token\n",
        "# Get your Hugging Face token from the environment variable\n",
        "token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "# If token is not found, raise an error\n",
        "if token is None:\n",
        "    raise ValueError(\"HUGGING_FACE_HUB_TOKEN environment variable not set. Please set it to your Hugging Face token.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FewHbBI3tcJ8"
      },
      "outputs": [],
      "source": [
        "#1. Download the model\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "\n",
        "llm_model = MllamaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    token=token,\n",
        "    temperature = 0.1\n",
        ")\n",
        "\n",
        "# 2. Set up the processor: the processor is used to handle different types of inputs (images and queries).\n",
        "llm_processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYtwo2F2hMux"
      },
      "outputs": [],
      "source": [
        "#3. Initialize the streamer\n",
        "streamer = TextStreamer(llm_processor.tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Who is Kaizad ? \"\n",
        "# qs = queries_embedding(query)\n",
        "# images_paths = retriever(qs = qs, collection_name=collection_name)\n",
        "# images_paths"
      ],
      "metadata": {
        "id": "gT-MpPPtJYjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_paths = ['/content/BUSINESS REQUIREMENT DOCUMENT v2 110420241840_page-0001.jpg',\n",
        "                '/content/BUSINESS REQUIREMENT DOCUMENT v2 110420241840_page-0002.jpg',\n",
        "                '/content/BUSINESS REQUIREMENT DOCUMENT v2 110420241840_page-0003.jpg']"
      ],
      "metadata": {
        "id": "Srl0kdjbLYsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kNVhGqWElmS"
      },
      "source": [
        "##### 4.2.1. Inference Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ngrok\n",
        "!pip install pyngrok\n",
        "!pip install uvicorn"
      ],
      "metadata": {
        "id": "bn1-BhC6KbCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SspRqjoxumi6"
      },
      "outputs": [],
      "source": [
        "def vrag(query):\n",
        "\n",
        "  images_ = []\n",
        "  collection_name = \"VRAG_BRD_Content\"\n",
        "  qs = queries_embedding([query])\n",
        "  images_paths_, scores = retriever(qs = qs, collection_name=collection_name)\n",
        "\n",
        "  for image_path_ in images_paths_:\n",
        "    images_.append(Image.open(image_path_))\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "      \"content\": [{\"type\": \"image\"},{\"type\": \"image\"},{\"type\": \"image\"},{\"type\": \"text\", \"text\": query}]\n",
        "          }\n",
        "  ]\n",
        "\n",
        "  input_text = llm_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "  inputs = llm_processor(\n",
        "      images_,\n",
        "      input_text,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\",\n",
        "  ).to(llm_model.device)\n",
        "  output =llm_model.generate(**inputs, max_new_tokens=256,  streamer=streamer)\n",
        "  answer = {\"answer\": llm_processor.decode(output[0],skip_special_tokens=True, skip_prompt = True)}\n",
        "  return answer, images_paths_, scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer, images_paths, scores = vrag(\"\")"
      ],
      "metadata": {
        "id": "C8bzORxUIdjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "tjcFR2MTc4Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(answer['answer']))"
      ],
      "metadata": {
        "id": "5OzOwQBY5SJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "class Item(BaseModel):\n",
        "    query: str"
      ],
      "metadata": {
        "id": "tjIrhRqTH1Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "ifpCWPZSKAgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post('/vrag_llama_11b')\n",
        "def api(item: Item):\n",
        "    # Call your function and get the result\n",
        "    answer = vrag(item.query)\n",
        "    return answer  # Return the result as JSON"
      ],
      "metadata": {
        "id": "fFTGPycnH7yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "PAWHZ9daKl4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken ****************************************************\n",
        "# Allow nested asyncio loops\n",
        "nest_asyncio.apply()\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "id": "hyR998bSJ_WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbszfTrOilP0"
      },
      "outputs": [],
      "source": [
        "# # Main loop\n",
        "# while True:\n",
        "#     query = input(\"Enter your query (or type 'exit' to quit): \")\n",
        "#     if query.lower() == \"exit\":\n",
        "#         print(\"Exiting the loop.\")\n",
        "#         break\n",
        "\n",
        "#     # Step 1: Embed the query\n",
        "#     qs = queries_embedding(query)\n",
        "\n",
        "#     # Step 2: Retrieve similar images based on the query embedding\n",
        "#     images_paths = retriever(qs = qs, collection_name=\"VRAG_BRD_Content\")\n",
        "#     for i in images_paths:\n",
        "#       img = Image.open(i)\n",
        "#       display(img)\n",
        "#       print(\"*\"*150)\n",
        "#       print(\"*\"*150)\n",
        "\n",
        "#     # Step 3: Process and prepare input for the model\n",
        "#     model_input = input_processing(query = query, images_paths = images_paths)\n",
        "\n",
        "#     # Step 4: Feed the input to your model (pseudo code here)\n",
        "#     model_output = llm_model.generate(**model_input, max_new_tokens=128,  streamer=streamer)  # Replace with your actual model prediction\n",
        "\n",
        "#     # Display or process the model output as needed\n",
        "#     display(Markdown(llm_processor.decode(model_output[0],skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfN8FbSWFpZ4"
      },
      "outputs": [],
      "source": [
        "# # Load the Llama 3.2 11B Vision model from Hugging Face\n",
        "# def load_model():\n",
        "#     model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "#     processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "#     # Get your Hugging Face token from the environment variable\n",
        "#     token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "\n",
        "#     # If token is not found, raise an error\n",
        "#     if token is None:\n",
        "#         raise ValueError(\"HUGGING_FACE_HUB_TOKEN environment variable not set. Please set it to your Hugging Face token.\")\n",
        "\n",
        "#     model = MllamaForConditionalGeneration.from_pretrained(\n",
        "#         model_id,\n",
        "#         torch_dtype=torch.bfloat16,\n",
        "#         device_map=\"auto\",\n",
        "#         token=token,\n",
        "#     )\n",
        "\n",
        "#     return model, processor\n",
        "\n",
        "# # Test the model with an image\n",
        "# def test_model(model, processor, image_path, query):\n",
        "\n",
        "#     # Initialize the streamer\n",
        "#     streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "#     # Load the image\n",
        "#     image = Image.open(image_path)\n",
        "\n",
        "#     # Prepare input for the model\n",
        "#     messages = [\n",
        "#         {\"role\": \"user\", \"content\": [\n",
        "#             {\"type\": \"image\"},\n",
        "#             {\"type\": \"text\", \"text\": query}\n",
        "#         ]}\n",
        "#     ]\n",
        "#     input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "#     inputs = processor(\n",
        "#         image,\n",
        "#         input_text,\n",
        "#         add_special_tokens=False,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(model.device)\n",
        "\n",
        "#     # Generate output\n",
        "#     output = model.generate(**inputs, max_new_tokens=128,  streamer=streamer)\n",
        "\n",
        "#     # Decode and print the output\n",
        "#     return output\n",
        "#     # print(processor.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo_sXNaQFwGh"
      },
      "outputs": [],
      "source": [
        "# # Load the model and processor\n",
        "# llm_model, llm_processor = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMqOwaVl6stI"
      },
      "outputs": [],
      "source": [
        "# queries[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E8ADtaCGC1r"
      },
      "outputs": [],
      "source": [
        "# # Specify an image to test (local path in Colab)\n",
        "# image_path = images_paths[0]\n",
        "# # Test the model with the specified image\n",
        "# query = queries[0]\n",
        "# output = test_model(llm_model, llm_processor, image_path, query = query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EZ1oo1V0FQZ9",
        "HNXufwyiFQZ_",
        "JAqRvJ3BFQaA",
        "m6vz1ZvlFQaI",
        "eGNR3STKFQaJ",
        "Sg9fwx85gzEw"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}